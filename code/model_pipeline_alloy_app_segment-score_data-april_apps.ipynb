{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path variables\n",
    "import sys\n",
    "project_path = '/Users/naresh/Downloads/DS/growth/nsl_v2/nsl_v2_final/'\n",
    "sys.path.insert(0, project_path+'config')\n",
    "from config import SQLQuery\n",
    "\n",
    "# core libraries\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from model_evaluations import model_metrics, cross_validation\n",
    "from model_building import tune_hyperparameters\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, precision_score, recall_score, roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from stability_monitoring import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# north star customers definition\n",
    "txn_days = 90\n",
    "txn_credit_amount = 15000\n",
    "\n",
    "# from_date = date(2022,10,1)\n",
    "# to_date = date(2023,2,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = SQLQuery('snowflake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15689, 164)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query to fetch the required data\n",
    "df_raw_app = q(\"\"\"with \n",
    "\n",
    "ALLOY_PERSONS as -- get all the alloy persons data (max of application_versions_id in case of duplicates)\n",
    "(\n",
    "  select * from (\n",
    "select * ,\n",
    "       row_number() over (partition by APPLICATION_ID order by APPLICATION_VERSION_ID desc) as rank\n",
    "from \"PROD_DB\".\"DATA\".\"ALLOY_EVALUATIONS_PERSONS\") \n",
    "where rank=1\n",
    "),\n",
    "\n",
    "APPLICATIONS as -- get all entries from applications table\n",
    "(\n",
    "  select *,\n",
    "    date(APPLICATION_COMPLETE_DATETIME) as app_comp_date, \n",
    "    month(APPLICATION_COMPLETE_DATETIME) as app_comp_month,\n",
    "    year(APPLICATION_COMPLETE_DATETIME) as app_comp_year\n",
    "  from \"PROD_DB\".\"DATA\".\"APPLICATIONS\"\n",
    "  where date(APPLICATION_COMPLETE_DATETIME) between '2023-04-01' and '2023-04-30'\n",
    "\n",
    ")\n",
    "\n",
    "    select A.*, P.*\n",
    "    from APPLICATIONS A\n",
    "    inner join ALLOY_PERSONS P\n",
    "    on A.application_id = P.application_id\n",
    "    \"\"\")\n",
    "\n",
    "df_raw_app.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    15689\n",
       "Name: ns_flag, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop null application ids\n",
    "df_raw_app = df_raw_app.dropna(subset='application_id')\n",
    "\n",
    "# tag north star customers\n",
    "df_raw_app['ns_flag'] = 1\n",
    "\n",
    "# get non-ns vs ns accounts split\n",
    "df_raw_app['ns_flag'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15689, 163)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicate columns\n",
    "df_raw_app = df_raw_app.loc[:,~df_raw_app.columns.duplicated()].copy()\n",
    "df_raw_app.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Applications with $0 deposit:', df_raw_app[df_raw_app['total_deposit'] == 0].shape[0])\n",
    "# print('Applications with deposit less than 5K :', df_raw_app[(df_raw_app['total_deposit'] <= 5000)].shape[0])\n",
    "# print('Applications with $15K deposit or more:', df_raw_app[df_raw_app['total_deposit'] >= txn_credit_amount].shape[0])\n",
    "# print('Applications with deposit between $0 and $15K :', df_raw_app[(df_raw_app['total_deposit'] > 0) & (df_raw_app['total_deposit'] < txn_credit_amount)].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "file = 'app_scoring_dataset_apps.pkl'\n",
    "path = project_path + 'data/'\n",
    "df_raw_app.reset_index(inplace=True)\n",
    "df_raw_app.to_pickle(path+file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_raw_data = q(\n",
    "\"\"\"\n",
    "with \n",
    "BUSINESS_DETAIL as -- get all business accounts created between Oct 2022 and Feb 2023\n",
    "(\n",
    "  select a.*\n",
    "  from \"PROD_DB\".\"DATA\".\"BUSINESSES\" a\n",
    "  inner join\n",
    "  \"PROD_DB\".\"DATA\".\"APPLICATIONS\" b\n",
    "  on a.application_id=b.application_id\n",
    "  where 1=1 \n",
    "        and date(a.ACCOUNT_CREATE_DATE) between '2023-04-01' and '2023-04-30'\n",
    "\n",
    ")\n",
    "\n",
    ",segment_all as (\n",
    "select b.application_id, a.USER_ID, a.anonymous_id, a.CONTEXT_IP, a.OWNER_ID, context_page_path, screen_width, screen_height, timezone, sent_at, received_at\n",
    "from BUSINESS_DETAIL b\n",
    "left join SEGMENT_DB.ONBOARDING_PROD.PAGES a\n",
    "on a.application_id=b.application_id\n",
    "order by a.application_id, received_at asc\n",
    ")\n",
    "\n",
    "-- Pull all the records which crossed the 13th question  \n",
    ",segment_till_incoming as (select a.application_id, a.context_page_path, a.received_at from \n",
    "(select a.application_id, a.context_page_path, a.received_at, rank() over(partition by a.application_id order by a.received_at asc) as rk\n",
    "from segment_all a\n",
    "where a.context_page_path='/app/business-questions/incoming'\n",
    ") a where rk=1 )\n",
    "\n",
    "-- Pull customer visited pages till the 13th question\n",
    ",final as (select\n",
    "a.application_id, a.USER_ID, a.anonymous_id, a.CONTEXT_IP, a.OWNER_ID, a.context_page_path, a.screen_width, a.screen_height, a.timezone, a.sent_at, a.received_at \n",
    "from segment_all a\n",
    "inner join segment_till_incoming b\n",
    "on a.application_id=b.application_id and a.received_at <= b.received_at\n",
    "order by a.application_id, a.received_at asc\n",
    ")\n",
    "\n",
    "select * from final order by application_id, received_at asc\n",
    "\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features_1(df:pd.DataFrame, cols_list:list, app_level_df:pd.DataFrame, training:bool=True):\n",
    "    \"\"\" \n",
    "        df : segment raw data\n",
    "        cols_list: columns list on which the operations are performed\n",
    "        app_level_df: application level data\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    new_cols = []\n",
    "    for col in cols_list:\n",
    "        tmp = df[['application_id',col]].drop_duplicates(subset=['application_id',col], keep='first')\n",
    "        tmp2 = pd.DataFrame(tmp.application_id.value_counts()).reset_index()\n",
    "        col = col+'_count'\n",
    "        tmp2.rename(columns={'application_id':col, 'index':'application_id'}, inplace=True)\n",
    "    \n",
    "        # Merging with app level df\n",
    "        app_level_df = pd.merge(app_level_df, tmp2, on='application_id', how='left')\n",
    "        new_cols.append(col)\n",
    "        \n",
    "    if training:\n",
    "        df_impute_segment = app_level_df[new_cols].median()\n",
    "        df_impute_segment = pd.DataFrame(df_impute_segment, columns=['impute_value']).reset_index().rename(columns={'index':'feature'})\n",
    "        df_impute_segment.to_pickle(project_path+'models/df_impute_segment.pkl') # Save the impute values as df\n",
    "\n",
    "    else:\n",
    "        df_impute_segment = pd.read_pickle(project_path+'models/df_impute_segment.pkl') # Load the impute values as df\n",
    "\n",
    "    # screen height and width features\n",
    "    tmp2 = pd.DataFrame()\n",
    "    tmp2 = pd.concat([tmp2, df[['application_id','screen_height','screen_width']]])\n",
    "    tmp2.drop_duplicates(subset=['application_id','screen_height','screen_width'],keep='first', inplace=True)\n",
    "\n",
    "    new_cols2 = []\n",
    "    list_value_type = ['max','min','mean','median']\n",
    "    for value_type in list_value_type:\n",
    "        colw = 'screen_width'\n",
    "        tmp = pd.DataFrame(tmp2.groupby(['application_id'])[colw].agg([value_type]))\n",
    "        tmp[value_type] = tmp[value_type].astype('float')\n",
    "        tmp.rename(columns={value_type:colw+'_'+value_type}, inplace=True) \n",
    "        app_level_df = pd.merge(app_level_df, tmp, on='application_id', how='left')\n",
    "        \n",
    "        new_cols2.append(colw+'_'+value_type)\n",
    "\n",
    "        colh = 'screen_height'\n",
    "        tmp = pd.DataFrame(tmp2.groupby(['application_id'])[colh].agg([value_type]))\n",
    "        tmp[value_type] = tmp[value_type].astype('float')\n",
    "        tmp.rename(columns={value_type:colh+'_'+value_type}, inplace=True) \n",
    "        app_level_df = pd.merge(app_level_df, tmp, on='application_id', how='left')\n",
    "        \n",
    "        new_cols2.append(colh+'_'+value_type)\n",
    "        \n",
    "    if training:\n",
    "        df_impute_segment2 = app_level_df[new_cols2].median()\n",
    "        df_impute_segment2 = pd.DataFrame(df_impute_segment2, columns=['impute_value']).reset_index().rename(columns={'index':'feature'})\n",
    "        df_impute_segment2.to_pickle(project_path+'models/df_impute_segment2.pkl') # Save the impute values as df\n",
    "\n",
    "    else:\n",
    "        df_impute_segment2 = pd.read_pickle(project_path+'models/df_impute_segment2.pkl') # Load the impute values as df\n",
    "\n",
    "    return app_level_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features_2(df:pd.DataFrame, app_level_df:pd.DataFrame, training:bool=True):\n",
    "    \"\"\" \n",
    "        df : segment raw data\n",
    "        cols_list: columns list on which the operations are performed\n",
    "        app_level_df: application level data\n",
    "        \n",
    "    \"\"\"\n",
    "    # Cleaning the list of owner_ids\n",
    "    def clean_string(x):\n",
    "        if x != None:\n",
    "            x = str(x)\n",
    "            x = [val.strip(\"\"\"'|[| |\"|,|]\"\"\") for val in x.split('/') if not val.strip(\"\"\"'|[| |\"|,|]\"\"\") in ['',None] and len(val.strip(\"\"\"'|[| |\"|,|]\"\"\"))<36]\n",
    "        return '/'.join(x)\n",
    "\n",
    "    df['context_page_path_clean'] = df['context_page_path'].apply(clean_string)\n",
    "    # One-hot encoding the unique page paths\n",
    "    col = 'context_page_path_clean'\n",
    "    tmp = df[col].str.get_dummies()\n",
    "    tmp['application_id'] = df.application_id\n",
    "    # Common pages that every applicant must visit\n",
    "    common_pages = ['verify-email-otp','welcome','app/applicant/personal-info','app/applicant/phone'\n",
    "    ,'app/applicant/otp-verify','app/applicant/address','app/applicant/dob-ssn','app/business/business-type'\n",
    "    ,'app/business/address','app/business/other-info','app/business-questions/about-business'\n",
    "    ,'app/business-questions/incoming']\n",
    "\n",
    "    # no.of visits per page\n",
    "    tmp2 = tmp[common_pages+['application_id']].groupby(['application_id']).sum()\n",
    "    tmp2.reset_index(drop=False, inplace=True)\n",
    "    tmp2 = pd.merge(app_level_df, tmp2, on='application_id', how='left')\n",
    "    \n",
    "    # no.of unique pages per user\n",
    "    tmp3 = tmp.groupby(['application_id']).sum()\n",
    "    tmp3.reset_index(drop=False, inplace=True)\n",
    "    tmp3 = pd.merge(tmp3, app_level_df[['application_id']], on='application_id', how='left')\n",
    "    drop_cols = ['404','app/applicant','app/business','application-denied','forgot-password','signup','status','undefined']\n",
    "    drop_cols = [col for col in tmp3.columns.to_list() if col in drop_cols]\n",
    "    tmp3 = tmp3.drop(columns=drop_cols)\n",
    "    tmp3 = tmp3.dropna(subset=['application_id'])\n",
    "    \n",
    "    tmp4 = pd.DataFrame()\n",
    "    tmp4['application_id'] = tmp3.application_id\n",
    "    cols = tmp3.drop(columns=['application_id']).columns.to_list()\n",
    "    tmp4[cols] = pd.DataFrame(np.where(tmp3[cols]>=1, 1, 0))\n",
    "    tmp4 = tmp4.set_index('application_id')\n",
    "    tmp4 = pd.DataFrame(tmp4.sum(axis=1), columns=['page_count'])\n",
    "    tmp2 = pd.merge(tmp2,tmp4,on='application_id',how='left')\n",
    "\n",
    "    if training:\n",
    "        df_impute_segment3 = tmp2[common_pages+['page_count']].median()\n",
    "        df_impute_segment3 = pd.DataFrame(df_impute_segment3, columns=['impute_value']).reset_index().rename(columns={'index':'feature'})\n",
    "        df_impute_segment3.to_pickle(project_path+'models/df_impute_segment3.pkl') # Save the impute values as df\n",
    "\n",
    "    else:\n",
    "        df_impute_segment3 = pd.read_pickle(project_path+'models/df_impute_segment3.pkl') # Load the impute values as df\n",
    "\n",
    "    return tmp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features_3(app_level_df:pd.DataFrame):\n",
    "    \"\"\" \n",
    "        df : segment raw data\n",
    "        cols_list: columns list on which the operations are performed\n",
    "        app_level_df: application level data\n",
    "        \n",
    "    \"\"\"\n",
    "    impute1 = pd.read_pickle(project_path+'models/df_impute_segment.pkl')\n",
    "    impute2 = pd.read_pickle(project_path+'models/df_impute_segment2.pkl')\n",
    "    impute3 = pd.read_pickle(project_path+'models/df_impute_segment3.pkl')\n",
    "    \n",
    "    ############# PART-1 #############\n",
    "    # Function to fill nulls with median values\n",
    "    def fill_null_values(df_impute:pd.DataFrame, data_df:pd.DataFrame,):\n",
    "        df_dict = dict(df_impute.values)\n",
    "        impute_cols = df_impute['feature'].to_list()\n",
    "        for col in data_df.columns.to_list():\n",
    "            if col in impute_cols:\n",
    "                data_df[col] = data_df[col].fillna(df_dict[col])\n",
    "                impute_cols.remove(col)\n",
    "        return data_df\n",
    "\n",
    "    # Filling nulls with median\n",
    "    app_level_df = fill_null_values(impute1, app_level_df)\n",
    "    app_level_df['sh_sw_ratio_count'] = app_level_df['screen_height_count']/app_level_df['screen_width_count']\n",
    "    app_level_df['sh_sw_ratio_count'] = app_level_df['sh_sw_ratio_count'].astype('float')\n",
    "    \n",
    "    cols_list_1 = impute1.feature.to_list()\n",
    "    # Feature Engg\n",
    "    for col in cols_list_1:\n",
    "        app_level_df[col] = np.where(app_level_df[col]==1,1,0)\n",
    "        \n",
    "\n",
    "    ############# PART-2 #############\n",
    "    # Filling nulls with median\n",
    "    app_level_df = fill_null_values(impute2, app_level_df)\n",
    "\n",
    "    # Function to fill zero screen width and height with median values\n",
    "    def fill_zero_values(df_impute:pd.DataFrame, data_df:pd.DataFrame,):\n",
    "        df_dict = dict(df_impute.values)\n",
    "        impute_cols = df_impute['feature'].to_list()\n",
    "        for col in data_df.columns.to_list():\n",
    "            if col in impute_cols:\n",
    "                data_df[col] = np.where(data_df[col]==0, df_dict[col], data_df[col])\n",
    "                impute_cols.remove(col)\n",
    "        return data_df\n",
    "    \n",
    "    # Filling zeros with median\n",
    "    app_level_df = fill_zero_values(impute2, app_level_df)\n",
    "    # Creating Ratios\n",
    "    list_value_type = ['max','min','mean','median']\n",
    "    for value_type in list_value_type:\n",
    "        colh = 'screen_height'\n",
    "        colw = 'screen_width'\n",
    "        app_level_df['sh_sw_ratio_'+value_type] = app_level_df[colh+'_'+value_type]/app_level_df[colw+'_'+value_type]\n",
    "\n",
    "    \n",
    "    ############# PART-3 #############\n",
    "    cols_list_3 = impute3.feature.to_list()\n",
    "    app_level_df = fill_null_values(impute3, app_level_df)\n",
    "    \n",
    "    return app_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_pages_oot = segment_raw_data.copy()\n",
    "# Filter only the existing applications in segment data\n",
    "segment_pages_oot = segment_pages_oot[~segment_pages_oot.received_at.isnull()]\n",
    "segment_pages_oot.reset_index(drop=True, inplace=True)\n",
    "segment_pages_oot = segment_pages_oot.sort_values(by=['application_id','received_at'])\n",
    "\n",
    "# Change data type\n",
    "cols = ['application_id','user_id','owner_id','anonymous_id','context_page_path','timezone']\n",
    "segment_pages_oot[cols] = segment_pages_oot[cols].astype('string')\n",
    "segment_pages_oot[['screen_width','screen_height']] = segment_pages_oot[['screen_width','screen_height']].astype('int')\n",
    "\n",
    "for col in segment_pages_oot.columns:\n",
    "    if col != 'user_id':\n",
    "        idx = segment_pages_oot.index[segment_pages_oot[col].isnull()].tolist()\n",
    "        idx.extend(segment_pages_oot.index[segment_pages_oot[col].isna()].tolist())\n",
    "        idx.extend(segment_pages_oot.index[segment_pages_oot[col] == ''].tolist())\n",
    "        idx.extend(segment_pages_oot.index[segment_pages_oot[col] == '[]'].tolist())\n",
    "        idx = list(set(idx))\n",
    "        segment_pages_oot.loc[idx, col] = None    \n",
    "\n",
    "\n",
    "df_oot = df_raw_app.copy()\n",
    "\n",
    "df_oot_tmp = pd.merge(segment_pages_oot, df_oot[['application_id', 'ns_flag']], on='application_id', how='inner')\n",
    "x_oot = df_oot_tmp.reset_index(drop=True)\n",
    "\n",
    "cols_list = ['timezone','user_id','owner_id','anonymous_id','context_ip','screen_width','screen_height']\n",
    "# Creating df with all apps\n",
    "app_level_data = pd.DataFrame()\n",
    "app_level_data['application_id'] = x_oot.application_id.unique()\n",
    "\n",
    "app_level_data = segment_features_1(df=x_oot, cols_list=cols_list, app_level_df=app_level_data, training=False)\n",
    "app_level_data = segment_features_2(df=x_oot, app_level_df=app_level_data, training=False)\n",
    "\n",
    "df_oot = pd.merge(df_oot[['application_id', 'ns_flag']], app_level_data, on='application_id', how='left')\n",
    "x_oot = df_oot.reset_index(drop=True)\n",
    "y_oot = x_oot['ns_flag']\n",
    "app_level_data_oot = segment_features_3(app_level_df=x_oot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the oot dataset\n",
    "file = 'segment_scoring_dataset_apps.pkl'\n",
    "path = project_path + 'data/'\n",
    "app_level_data_oot.reset_index(inplace=True)\n",
    "app_level_data_oot.to_pickle(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nulls_to_one_format(df:pd.DataFrame):\n",
    "    for col in df.columns:\n",
    "        idx = df.index[df[col].isnull()].tolist()\n",
    "        idx.extend(df.index[df[col].isna()].tolist())\n",
    "        idx.extend(df.index[df[col] == ''].tolist())\n",
    "        idx.extend(df.index[df[col] == '[]'].tolist())\n",
    "        idx = list(set(idx))\n",
    "        df.loc[idx, col] = None\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_null_values(df_impute:pd.DataFrame, data_df:pd.DataFrame,):\n",
    "    df_dict = dict(df_impute.values)\n",
    "    impute_cols = df_impute['feature'].to_list()\n",
    "    for col in data_df.columns.to_list():\n",
    "        if col in impute_cols:\n",
    "            data_df[col] = data_df[col].fillna(df_dict[col])\n",
    "            impute_cols.remove(col)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_alloy(df:pd.DataFrame, training:bool=True):\n",
    "    \n",
    "    # feature 1\n",
    "    col = 'iovation_device_type'\n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = np.where(df[col].isin(['windows','iphone','mac','android']),df[col],'other')\n",
    "        \n",
    "    # feature 2\n",
    "    df['line_type'] = df['line_type'].str.lower()\n",
    "    df['line_type'] = np.where(df['line_type']=='mobile', 'mobile', 'other')\n",
    "\n",
    "    # feature 3\n",
    "    df['iovation_device_timezone'] = np.where(df['iovation_device_timezone'].isin(['300','360','480']),\n",
    "                                                   df['iovation_device_timezone'], 'other')\n",
    "    # feature 4 \n",
    "    col = 'iovation_device_ip_isp'\n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = np.where(df[col].str.contains('verizon'),'verizon',df[col])\n",
    "    df[col] = np.where(df[col].str.contains('at&t'),'att',df[col])\n",
    "    df[col] = np.where(df[col].str.match('att'),'att',df[col])\n",
    "    df[col] = np.where(df[col].str.match('charter [b|c]'),'charter',df[col])\n",
    "    df[col] = np.where(df[col].str.match('comcast'),'comcast',df[col])\n",
    "    df[col] = np.where(df[col].isin(['att','charter','verizon','comcast']),df[col],'other')\n",
    "    \n",
    "    # feature 5 \n",
    "    col = 'iovation_device_ip_org'\n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = np.where(df[col].str.contains('verizon'),'verizon',df[col])\n",
    "    df[col] = np.where(df[col].str.contains('at&t'),'att',df[col])\n",
    "    df[col] = np.where(df[col].str.match('att'),'att',df[col])\n",
    "    df[col] = np.where(df[col].str.match('charter [b|c]'),'charter',df[col])\n",
    "    df[col] = np.where(df[col].str.match('comcast'),'comcast',df[col])\n",
    "    df[col] = np.where(df[col].isin(['att','charter','verizon','comcast']),df[col],'other')\n",
    "    \n",
    "    # feature 6 \n",
    "    col = 'carrier'\n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = np.where(df[col].str.contains('verizon'),'verizon',df[col])\n",
    "    df[col] = np.where(df[col].str.contains('at&t'),'att',df[col])\n",
    "    df[col] = np.where(df[col].str.match('att'),'att',df[col])\n",
    "    df[col] = np.where(df[col].str.contains('t-mobile*'),'tmobile',df[col])\n",
    "    df[col] = np.where(df[col].isin(['att','tmobile','verizon']),df[col],'other')\n",
    "    \n",
    "#     # feature 7 \n",
    "#     col = 'iovation_device_ip_region'\n",
    "#     df[col] = df[col].str.lower()\n",
    "#     # Bucketing into low and high deposit categories\n",
    "#     high_ns_region = pd.read_pickle(project_path+'models/high_ns_region.pkl')[col].to_list()\n",
    "#     low_ns_region = pd.read_pickle(project_path+'models/low_ns_region.pkl')[col].to_list()\n",
    "#     df[col] = np.where(df[col].isin(high_ns_region),'high_ns',df[col])\n",
    "#     df[col] = np.where(df[col].isin(low_ns_region),'low_ns',df[col])\n",
    "#     df[col] = np.where(df[col].isin(['high_ns','low_ns']),df[col],'other')\n",
    "    \n",
    "#     # feature 8\n",
    "#     col = 'iovation_device_ip_city'\n",
    "#     df[col] = df[col].str.lower()\n",
    "#     # Bucketing into low and high deposit categories\n",
    "#     high_ns_city = pd.read_pickle(project_path+'models/high_ns_city.pkl')[col].to_list()\n",
    "#     low_ns_city = pd.read_pickle(project_path+'models/low_ns_city.pkl')[col].to_list()\n",
    "#     df[col] = np.where(df[col].isin(high_ns_city),'high_ns',df[col])\n",
    "#     df[col] = np.where(df[col].isin(low_ns_city),'low_ns',df[col])\n",
    "#     df[col] = np.where(df[col].isin(['high_ns','low_ns']),df[col],'other')\n",
    "\n",
    "    # IDA reason codes\n",
    "    ida_reason_codes_columns = pd.read_pickle(project_path+'models/ida_reason_codes_columns.pkl')\n",
    "    df_tmp2 = pd.DataFrame(index=range(df.shape[0]),columns=ida_reason_codes_columns['feature'].to_list())\n",
    "    df_tmp2 = df_tmp2.fillna(0)\n",
    "\n",
    "    col = 'ida_score_reason_1'\n",
    "    df_tmp = pd.get_dummies(df[col].astype('int'), prefix=col)\n",
    "    col = 'ida_score_reason_2'\n",
    "    df_tmp = pd.concat([df_tmp, pd.get_dummies(df[col].astype('int'), prefix=col)], axis=1)\n",
    "    col = 'ida_score_reason_3'\n",
    "    df_tmp = pd.concat([df_tmp, pd.get_dummies(df[col].astype('int'), prefix=col)], axis=1)    \n",
    "    \n",
    "    df_tmp2.update(df_tmp)\n",
    "    df_tmp2 = df_tmp2.astype('int')\n",
    "    df = pd.concat([df,df_tmp2], axis=1)\n",
    "\n",
    "    # Socure reason codes\n",
    "    # Cleaning the list of reason codes\n",
    "    def clean_string(x):\n",
    "        if x != None:\n",
    "            x = [val.strip(\"\"\"'|[| |\"|,|]\"\"\") for val in x.split('\\n') if not val.strip(\"\"\"'|[| |\"|,|]\"\"\") in ['',None]]\n",
    "        return x\n",
    "\n",
    "    socure_cols = ['socure_phonerisk_reason_code','socure_addressrisk_reason_code','socure_emailrisk_reason_code'\n",
    "                  ,'socure_reason_code']\n",
    "    df_socure = df[socure_cols]\n",
    "\n",
    "    # Collecting the cleaned reason codes\n",
    "    for col in socure_cols:\n",
    "        df_socure[col] = df_socure[col].astype('str')\n",
    "        df_socure[col] = df_socure[col].str.lower()\n",
    "        df_socure[col] = df_socure[col].apply(clean_string)\n",
    "        \n",
    "    socure_cols = ['socure_phonerisk_reason_code','socure_addressrisk_reason_code','socure_emailrisk_reason_code'\n",
    "                  ,'socure_reason_code']\n",
    "    dict_cols = {}\n",
    "    for col in socure_cols:\n",
    "        df_socure = df_socure.drop(col, 1).join(df_socure[col].str.join('|').str.get_dummies())\n",
    "        socure_cols = df_socure.columns[df_socure.columns.str.startswith('socure')].to_list()\n",
    "        true_cols = list(set(df_socure.columns.to_list()) - set(socure_cols))\n",
    "        new_cols = []\n",
    "        for col2 in true_cols:\n",
    "            new_cols.append(col+'_'+col2)\n",
    "        dict_cols = dict(zip(true_cols, new_cols)) | dict_cols\n",
    "        df_socure.rename(columns=dict_cols, inplace=True)\n",
    "\n",
    "    df_socure = df_socure.T\n",
    "    df_socure = df_socure[~df_socure.index.duplicated(keep='first')].T\n",
    "    df_socure = df_socure.astype('int')\n",
    "        \n",
    "    socure_reason_codes_columns = pd.read_pickle(project_path+'models/socure_reason_codes_columns.pkl')\n",
    "    df_tmp = pd.DataFrame(index=range(df.shape[0]),columns=socure_reason_codes_columns['feature'].to_list())\n",
    "    df_tmp = df_tmp.fillna(0)\n",
    "    \n",
    "    df_tmp.update(df_socure)\n",
    "    df_tmp = df_tmp.astype('int')\n",
    "    df = pd.concat([df,df_tmp], axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_app(df:pd.DataFrame):\n",
    "    # business type\n",
    "    df['business_group'] = np.where(df['business_type'] == 'sole_proprietorship', 0, 1)\n",
    "\n",
    "    # email domain\n",
    "    df['email_domain_bucket'] = np.where(df['email_domain'].isin(\n",
    "                                    list(df.groupby(\"email_domain\").filter(lambda x: (len(x) >= 10))['email_domain'].unique())), 0, 1)\n",
    "\n",
    "    # estimated business numbers\n",
    "    estimated_cols = ['estimated_monthly_revenue',\n",
    "                      'incoming_ach_payments',\n",
    "                      'check_deposit_amount',\n",
    "                      'incoming_wire_transfer',\n",
    "                      'outgoing_ach_and_checks',\n",
    "                      'outgoing_wire_transfers']\n",
    "\n",
    "    # grouping all responses into 5K+ and 5K-\n",
    "    for col in estimated_cols:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = np.where(df[col].isin(['$5k +', '$50k +']), 1, 0)\n",
    "\n",
    "    # number of employees\n",
    "    df[\"number_of_employees\"] = pd.to_numeric(df[\"number_of_employees\"])\n",
    "    df['number_of_employees_bin'] = df[\"number_of_employees\"]\n",
    "    df['number_of_employees_bin'] = np.where((df['number_of_employees']>=2) & (df['number_of_employees']<=5), 2, \n",
    "                                    np.where((df['number_of_employees']>5), 3, 1))\n",
    "    df['number_of_employees_bin'].value_counts()\n",
    "\n",
    "    # purpose of account - options selected\n",
    "    df['purpose_of_account_options_selected'] = df['purpose_of_account'].str.count(',')+1\n",
    "    df['purpose_of_account_options_selected'] = np.where(df['purpose_of_account_options_selected']<=3, 0, 1)\n",
    "\n",
    "#     # industry type\n",
    "#     hdi_group = pd.read_pickle(project_path+'models/hdi_group.pkl')['value'].to_list()\n",
    "\n",
    "#     df['industry_group'] = np.where(df['industry_category_name'].isin(hdi_group), 1, 0)\n",
    "#     df['industry_group'].value_counts()\n",
    "\n",
    "    # business age\n",
    "    df['date_of_establishment_dt'] = pd.to_datetime(df['date_of_establishment'], format='%Y-%m', errors='coerce').dropna()\n",
    "    df['business_age'] = round((df['application_start_datetime'] - df['date_of_establishment_dt']) / np.timedelta64(1, 'Y'),2)\n",
    "    df['business_age'] = df['business_age'].fillna(0)\n",
    "    df['business_age_bucket'] = np.where(df['business_age'] <= 1, 1, \n",
    "                                        np.where((df['business_age']>1) & (df['business_age']<=3), 2, 3))\n",
    "    df['business_age_bucket'].value_counts().sort_values()\n",
    "\n",
    "    # website flag\n",
    "    df['website_flag'] = np.where(df['website'].isna(), 0, 1)\n",
    "\n",
    "#     # state\n",
    "#     hds = pd.read_pickle(project_path+'models/high_ns_busi_state.pkl')['business_address_state'].to_list()\n",
    "#     lds = pd.read_pickle(project_path+'models/low_ns_busi_state.pkl')['business_address_state'].to_list()        \n",
    "#     df['business_address_state'] = np.where(df['business_address_state'].isin(hds), 'high_ns', \n",
    "#                                            np.where(df['business_address_state'].isin(lds), 'low_ns', 'other'))\n",
    "\n",
    "#     # city\n",
    "#     hdc = pd.read_pickle(project_path+'models/high_ns_busi_city.pkl')['business_address_city'].to_list()\n",
    "#     ldc = pd.read_pickle(project_path+'models/low_ns_busi_city.pkl')['business_address_city'].to_list()\n",
    "#     df['business_address_city'] = np.where(df['business_address_city'].isin(hdc), 'high_ns', \n",
    "#                                        np.where(df['business_address_city'].isin(ldc), 'low_ns', 'other'))    \n",
    "    \n",
    "    # current bank\n",
    "    hdb_group = ['bluevine', 'other-national-bank', 'td-ank', 'chase', 'usaa']\n",
    "    df['current_bank_group'] = np.where(df['current_bank'].isin(hdb_group), 1, 0)\n",
    "\n",
    "    # website similar to email domain\n",
    "    df['website_processed'] = df['website'].fillna('')\n",
    "    df['website_email_domain_match'] = df.apply(lambda x: x.email_domain in x.website_processed, axis=1)\n",
    "    df['website_email_domain_match'] = np.where(df['website_email_domain_match']==True, 1, 0)\n",
    "\n",
    "    # ein_ssn\n",
    "    df['ein_ssn'] = np.where(df['ein_ssn']=='1', 1, 0)\n",
    "\n",
    "#     # business pitch\n",
    "#     bag_of_words = pd.read_pickle(project_path+'models/business_pitch_bow.pkl')['bow'].to_list()\n",
    "#     df['bow_flag'] = np.where(df['business_pitch_lema_spacy'].str.contains('|'.join(bag_of_words)), 1, 0)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_features_app = [\n",
    " 'application_start_datetime',\n",
    " 'email_domain',\n",
    " 'estimated_monthly_revenue',\n",
    " 'incoming_ach_payments',\n",
    " 'check_deposit_amount',\n",
    " 'incoming_wire_transfer',\n",
    " 'outgoing_ach_and_checks',\n",
    " 'outgoing_wire_transfers',\n",
    " 'number_of_employees',\n",
    " 'purpose_of_account',\n",
    " 'current_bank',\n",
    " 'industry_category_name',\n",
    " 'date_of_establishment',\n",
    " 'business_type',\n",
    " 'website',\n",
    " 'business_address_city',\n",
    " 'business_address_state',\n",
    " 'ein_ssn'\n",
    "#  'business_pitch_lema_spacy'\n",
    "]\n",
    "\n",
    "# Alloy Features\n",
    "# Numerical\n",
    "num_cols = ['socure_emailrisk', 'socure_phonerisk', 'socure_addressrisk','socure_sigma','sentilink_abuse_score'\n",
    "            ,'sentilink_first_party_synthetic_score','sentilink_third_party_synthetic_score','person_score'\n",
    "            ,'ida_score']\n",
    "# Categorical\n",
    "cat_columns = ['iovation_device_type','iovation_device_timezone','iovation_device_ip_isp'\n",
    "               ,'iovation_device_ip_city','iovation_device_ip_region'\n",
    "               ,'iovation_device_ip_org'\n",
    "               ,'carrier','line_type']\n",
    "# Reason codes - IDA\n",
    "ida_reason_cols = ['ida_score_reason_1','ida_score_reason_2','ida_score_reason_3']\n",
    "# Reason codes - Socure\n",
    "socure_reason_cols = ['socure_reason_code','socure_emailrisk_reason_code','socure_phonerisk_reason_code'\n",
    "                      ,'socure_addressrisk_reason_code']\n",
    "\n",
    "raw_features_segment = app_level_data_oot.drop(columns=['application_id','ns_flag','index']\n",
    "                                         ).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_features_alloy = num_cols + cat_columns + ida_reason_cols + socure_reason_cols\n",
    "raw_features = raw_features_app + raw_features_alloy + raw_features_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_features_app = ['estimated_monthly_revenue', \n",
    "                        'incoming_ach_payments', \n",
    "                        'check_deposit_amount', \n",
    "                        'incoming_wire_transfer',\n",
    "                        'outgoing_ach_and_checks', \n",
    "                        'outgoing_wire_transfers',\n",
    "                        'ein_ssn',\n",
    "                        'business_group', \n",
    "                        'email_domain_bucket', \n",
    "                        'number_of_employees_bin',\n",
    "                        'purpose_of_account_options_selected', \n",
    "#                         'industry_group',\n",
    "                        'industry_category_name',\n",
    "                        'business_age_bucket',\n",
    "                        'website_flag', \n",
    "                        'business_address_state', \n",
    "                        'business_address_city', \n",
    "                        'current_bank_group',\n",
    "                        'website_email_domain_match'\n",
    "#                         'bow_flag'\n",
    "                           ]\n",
    "\n",
    "# Alloy Features\n",
    "ida_reason_codes_columns = pd.read_pickle(project_path+'models/ida_reason_codes_columns.pkl')['feature'].to_list()\n",
    "socure_reason_codes_columns = pd.read_pickle(project_path+'models/socure_reason_codes_columns.pkl'\n",
    "                                            )['feature'].to_list()\n",
    "\n",
    "reason_codes_cols = ida_reason_codes_columns + socure_reason_codes_columns\n",
    "# Final alloy columns\n",
    "independent_features_alloy_tmp = num_cols + cat_columns + reason_codes_cols\n",
    "independent_features_alloy_tmp = [val for val in independent_features_alloy_tmp if val not in \n",
    "                                  ['iovation_device_ip_region','iovation_device_ip_city']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oot_df = df_raw_app.copy()\n",
    "y_oot = oot_df['ns_flag']\n",
    "\n",
    "# Adding segment test data\n",
    "segment_oot = app_level_data_oot.copy()\n",
    "oot_df = pd.merge(oot_df, segment_oot, on='application_id', how='inner')\n",
    "x_oot = oot_df[raw_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all string features to lowercase\n",
    "string_features = ['email_domain',\n",
    " 'purpose_of_account',\n",
    " 'current_bank',\n",
    " 'industry_category_name',\n",
    " 'business_type',\n",
    " 'business_address_city',\n",
    " 'business_address_state',\n",
    " 'website']+cat_columns+socure_reason_cols\n",
    "\n",
    "for col in string_features:\n",
    "    x_oot[col] = x_oot[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15689, 75), (15689,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oot.shape, y_oot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oot = convert_nulls_to_one_format(df=x_oot)\n",
    "df_impute = pd.read_pickle(project_path+'models/df_impute.pkl')\n",
    "x_oot = fill_null_values(df_impute, x_oot)\n",
    "\n",
    "x_oot = feature_engineering_app(df=x_oot)\n",
    "x_oot = feature_engineering_alloy(df=x_oot, training=False)\n",
    "\n",
    "# Removing reason code features as these are already encoded\n",
    "x_oot.drop(columns=ida_reason_cols+socure_reason_cols, inplace=True)\n",
    "\n",
    "# Encoding the categories\n",
    "x_object_cols = x_oot[independent_features_alloy_tmp].select_dtypes(include=['object']).columns.to_list()\n",
    "x_object_cols = x_object_cols+['industry_category_name']\n",
    "\n",
    "x_object_onehot = pd.get_dummies(x_oot[x_object_cols]) # create dummies\n",
    "x_object_onehot = x_object_onehot.astype('int')\n",
    "x_oot = pd.concat([x_oot.drop(columns=x_object_cols), x_object_onehot], axis=1)\n",
    "x_oot.columns= x_oot.columns.str.lower() # convert column names to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15689, 400)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the final independent features\n",
    "# independent_features_alloy = num_cols + x_object_onehot.columns.to_list() + reason_codes_cols\n",
    "# independent_features = independent_features_app + independent_features_alloy + raw_features_segment\n",
    "\n",
    "# x_oot = x_oot[independent_features]\n",
    "\n",
    "train_cols = pd.read_pickle(project_path+'models/train_data_columns.pkl')['feature'].to_list()\n",
    "df_tmp = pd.DataFrame(index=range(x_oot.shape[0]),columns=train_cols)\n",
    "df_tmp = df_tmp.fillna(0)\n",
    "df_tmp.update(x_oot)\n",
    "x_oot = df_tmp.copy()\n",
    "x_oot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15689, (15689,), 1.0, 0.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_oot), y_oot.shape, y_oot.mean(), y_oot[y_oot==0].shape[0]/y_oot.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = \"nsql_model_v2.pkl\"\n",
    "path = project_path + 'models/'\n",
    "xgb_model = pickle.load(open(path+file_name, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_features = [\n",
    "    'estimated_monthly_revenue', 'incoming_ach_payments',\n",
    "       'screen_width_mean', 'sh_sw_ratio_mean',\n",
    "       'socure_emailrisk_reason_code_i553', 'iovation_device_type_mac',\n",
    "       'industry_category_name_professional, scientific, and technical services',\n",
    "       'outgoing_ach_and_checks', 'business_group', 'socure_sigma',\n",
    "       'industry_category_name_real estate rental and leasing',\n",
    "       'industry_category_name_retail trade', 'socure_emailrisk',\n",
    "       'iovation_device_type_android', 'check_deposit_amount',\n",
    "       'socure_emailrisk_reason_code_i566',\n",
    "       'socure_phonerisk_reason_code_i630', 'socure_phonerisk',\n",
    "       'outgoing_wire_transfers', 'socure_emailrisk_reason_code_r561',\n",
    "       'socure_reason_code_r207', 'iovation_device_timezone_480',\n",
    "       'socure_phonerisk_reason_code_i614',\n",
    "       'socure_emailrisk_reason_code_i555',\n",
    "       'industry_category_name_manufacturing'\n",
    "]\n",
    "\n",
    "y_pred = xgb_model.predict(x_oot[top_features])\n",
    "predicted_probas = xgb_model.predict_proba(x_oot[top_features])\n",
    "# model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw_app['prob_v2'] = predicted_probas[:,1]\n",
    "df_raw_app2 = pd.concat([df_raw_app, x_oot], axis=1)\n",
    "df_raw_app2['v2_flag_sp'] = np.where(((df_raw_app2.business_group==0) & (df_raw_app2.prob_v2>=0.5)),1,0)\n",
    "df_raw_app2['v2_flag_nsp'] = np.where(((df_raw_app2.business_group==1) & (df_raw_app2.prob_v2>=0.3)),1,0)\n",
    "df_raw_app2['v2_flag'] = df_raw_app2['v2_flag_nsp'] + df_raw_app2['v2_flag_sp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15689, 568)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_app2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5819, 568), (9870, 568))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_app2[df_raw_app2.business_group==0].shape, df_raw_app2[df_raw_app2.business_group==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043478260869565216"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df_raw_app2[df_raw_app2.business_group==0]\n",
    "tmp.v2_flag.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5157041540020263"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df_raw_app2[df_raw_app2.business_group==1]\n",
    "tmp.v2_flag.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5343"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_app2.v2_flag.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'march' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xs/0qxx9s551gx4s8scyf4mv10r0000gn/T/ipykernel_68578/1743012620.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmarch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'march' is not defined"
     ]
    }
   ],
   "source": [
    "# march: sp ns_rate:0.04892579888066438, apps:5539 and nsp ns_rate:0.5111223458038423, apps:9890\n",
    "# april: sp ns_rate:0.04892579888066438, apps:5539 and nsp ns_rate:0.5111223458038423, apps:9890\n",
    "\n",
    "april: 5320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.120022\n",
       "1        0.273272\n",
       "2        0.468644\n",
       "3        0.142651\n",
       "4        0.151664\n",
       "           ...   \n",
       "48219    0.137243\n",
       "48220    0.283460\n",
       "48221    0.543686\n",
       "48222    0.093176\n",
       "48223    0.072942\n",
       "Name: prob_v2, Length: 48224, dtype: float32"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_app[business_t].prob_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72, 20, 57 and ns rate: 0.05877579411082773 and total: 25878 - nov'22 to feb'23 metrics\n",
    "# 72, 20, 57 and ns rate: 0.05640347098282971 and total: 32498 - oct'22 to feb'23 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ns_flag</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecileRank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0.0, 0.2]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19466</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.2, 0.4]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>14436</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.4, 0.6]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9443</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.6, 0.8]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4464</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.8, 1.0]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>415</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ns_flag  volume  volume%\n",
       "DecileRank                          \n",
       "(0.0, 0.2]      1.0   19466     0.40\n",
       "(0.2, 0.4]      1.0   14436     0.30\n",
       "(0.4, 0.6]      1.0    9443     0.20\n",
       "(0.6, 0.8]      1.0    4464     0.09\n",
       "(0.8, 1.0]      1.0     415     0.01"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oot.reset_index(drop=True,inplace=True)\n",
    "y_oot.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_oot = x_oot.copy()\n",
    "df_oot['ns_flag'] = y_oot\n",
    "df_oot['proba'] = xgb_model.predict_proba(x_oot[top_features])[:,1:].flatten()\n",
    "\n",
    "# custom_bins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "custom_bins = [0,0.2,0.4,0.6,0.8,1]\n",
    "\n",
    "df_oot['proba'] = np.round(df_oot['proba'], 3)\n",
    "df_oot['DecileRank']= pd.cut(df_oot['proba'], bins=custom_bins)\n",
    "df_stats = pd.DataFrame(df_oot.groupby(by='DecileRank')['ns_flag'].mean())\n",
    "df_stats['volume'] = df_oot.groupby(by='DecileRank')['ns_flag'].count()\n",
    "df_stats['volume%'] = np.round((df_stats['volume'] / df_stats['volume'].sum()), 2)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ns_flag</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecileRank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0.009000000000000001, 0.245]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>24227</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.245, 0.92]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23997</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ns_flag  volume  volume%\n",
       "DecileRank                                             \n",
       "(0.009000000000000001, 0.245]      1.0   24227      0.5\n",
       "(0.245, 0.92]                      1.0   23997      0.5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oot.reset_index(drop=True,inplace=True)\n",
    "y_oot.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_oot = x_oot.copy()\n",
    "df_oot['ns_flag'] = y_oot\n",
    "df_oot['proba'] = xgb_model.predict_proba(x_oot[top_features])[:,1:].flatten()\n",
    "\n",
    "# custom_bins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "# custom_bins = [0,0.2,0.4,0.6,0.8,1]\n",
    "\n",
    "df_oot['proba'] = np.round(df_oot['proba'], 3)\n",
    "df_oot['DecileRank']= pd.qcut(df_oot['proba'], q=2)\n",
    "df_stats = pd.DataFrame(df_oot.groupby(by='DecileRank')['ns_flag'].mean())\n",
    "df_stats['volume'] = df_oot.groupby(by='DecileRank')['ns_flag'].count()\n",
    "df_stats['volume%'] = np.round((df_stats['volume'] / df_stats['volume'].sum()), 2)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stats.to_csv(project_path+'rough.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05877579411082773"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oot.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25878,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Precision vs Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_threshold_plot(y_true:np.array, yhat:np.array, thresholds:list):\n",
    "    # keep probabilities for the positive outcome only\n",
    "    precision_vals = []\n",
    "    recall_vals = []\n",
    "    for threshold in thresholds:\n",
    "        yhat2 = []\n",
    "        for val in yhat:\n",
    "            if val>=threshold:\n",
    "                yhat2.append(1)\n",
    "            else:\n",
    "                yhat2.append(0)\n",
    "        yhat2 = np.array(yhat2)    \n",
    "        pr = precision_score(y_true, yhat2, average='binary')\n",
    "        rec = recall_score(y_true, yhat2, average='binary')\n",
    "        roc_auc = roc_auc_score(y_true, yhat2, average=None)\n",
    "        \n",
    "        print('threshold:', threshold, ' predicted:', sum(yhat2), ' pred NS Rate:', np.round(yhat2.mean(),3),\n",
    "             ' precision:', np.round(pr,3), ' recall:',np.round(rec,3), ' auc:',np.round(roc_auc,3))\n",
    "        precision_vals.append(pr)\n",
    "        recall_vals.append(rec)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(precision_vals, recall_vals, marker='.')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('Precision')\n",
    "    pyplot.ylabel('Recall')\n",
    "    pyplot.grid(True)\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "# non-sp = 1\n",
    "x_data = x_train.copy()\n",
    "y_data = y_train.copy()\n",
    "yhat = tuned_model.predict_proba(x_data[x_data.business_group==1])[:,1:].flatten()\n",
    "y_true = y_data[y_data.index.isin(x_data[x_data.business_group==1].index.to_list())]\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "print('Data Size:',y_true.shape[0],' True DS NS:',sum(y_true))    \n",
    "precision_recall_threshold_plot(y_true=y_true, yhat=yhat, thresholds=thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "# non-sp = 1\n",
    "x_data = x_test.copy()\n",
    "y_data = y_test.copy()\n",
    "yhat = tuned_model.predict_proba(x_data[x_data.business_group==1])[:,1:].flatten()\n",
    "y_true = y_data[y_data.index.isin(x_data[x_data.business_group==1].index.to_list())]\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "precision_recall_threshold_plot(y_true=y_true, yhat=yhat, thresholds=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "# non-sp = 1\n",
    "x_data = x_oot.copy()\n",
    "y_data = y_oot.copy()\n",
    "yhat = tuned_model.predict_proba(x_data[x_data.business_group==1])[:,1:].flatten()\n",
    "y_true = y_data[y_data.index.isin(x_data[x_data.business_group==1].index.to_list())]\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "precision_recall_threshold_plot(y_true=y_true, yhat=yhat, thresholds=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "# sp = 0\n",
    "x_data = x_train.copy()\n",
    "y_data = y_train.copy()\n",
    "yhat = tuned_model.predict_proba(x_data[x_data.business_group==0])[:,1:].flatten()\n",
    "y_true = y_data[y_data.index.isin(x_data[x_data.business_group==0].index.to_list())]\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "precision_recall_threshold_plot(y_true=y_true, yhat=yhat, thresholds=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "# sp = 0\n",
    "x_data = x_test.copy()\n",
    "y_data = y_test.copy()\n",
    "yhat = tuned_model.predict_proba(x_data[x_data.business_group==0])[:,1:].flatten()\n",
    "y_true = y_data[y_data.index.isin(x_data[x_data.business_group==0].index.to_list())]\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "precision_recall_threshold_plot(y_true=y_true, yhat=yhat, thresholds=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "# sp = 0\n",
    "x_data = x_oot.copy()\n",
    "y_data = y_oot.copy()\n",
    "yhat = tuned_model.predict_proba(x_data[x_data.business_group==0])[:,1:].flatten()\n",
    "y_true = y_data[y_data.index.isin(x_data[x_data.business_group==0].index.to_list())]\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "precision_recall_threshold_plot(y_true=y_true, yhat=yhat, thresholds=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_imp_df = pd.DataFrame(zip(x_train.columns.to_list(), tuned_model.feature_importances_), columns=['feature','importance'])\n",
    "feature_imp_df = feature_imp_df.sort_values(by=['importance'], ascending=False).reset_index(drop=True)\n",
    "feature_imp_df[feature_imp_df.importance>=0.005]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_df[feature_imp_df.importance>0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(x_train[raw_features_segment].corr()).to_csv(project_path+'segment_features_correlation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,40)\n",
    "plot_importance(tuned_model, importance_type='total_gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train rank ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "x_train.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_train = x_train.copy()\n",
    "df_train['ns_flag'] = y_train\n",
    "df_train['proba'] = tuned_model.predict_proba(x_train)[:,1:].flatten()\n",
    "\n",
    "# custom_bins = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "custom_bins = [0.0, 0.2, 0.4, 0.6, 1.0]\n",
    "df_train['proba'] = np.round(df_train['proba'], 3)\n",
    "df_train['DecileRank']= pd.cut(df_train['proba'], bins=custom_bins)\n",
    "df_stats = pd.DataFrame(df_train.groupby(by='DecileRank')['ns_flag'].mean())\n",
    "df_stats['volume'] = df_train.groupby(by='DecileRank')['ns_flag'].count()\n",
    "df_stats['volume%'] = np.round((df_stats['volume'] / df_stats['volume'].sum())*100, 2)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test rank ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "x_test.reset_index(drop=True,inplace=True)\n",
    "y_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_test = x_test.copy()\n",
    "df_test['ns_flag'] = y_test\n",
    "df_test['proba'] = tuned_model.predict_proba(x_test)[:,1:].flatten()\n",
    "\n",
    "df_test['proba'] = np.round(df_test['proba'], 3)\n",
    "df_test['DecileRank']= pd.cut(df_test['proba'], bins=custom_bins)\n",
    "df_stats = pd.DataFrame(df_test.groupby(by='DecileRank')['ns_flag'].mean())\n",
    "df_stats['volume'] = df_test.groupby(by='DecileRank')['ns_flag'].count()\n",
    "df_stats['volume%'] = np.round((df_stats['volume'] / df_stats['volume'].sum())*100, 2)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOT rank ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oot.reset_index(drop=True,inplace=True)\n",
    "y_oot.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_oot = x_oot.copy()\n",
    "df_oot['ns_flag'] = y_oot\n",
    "df_oot['proba'] = tuned_model.predict_proba(x_oot)[:,1:].flatten()\n",
    "\n",
    "df_oot['proba'] = np.round(df_oot['proba'], 3)\n",
    "df_oot['DecileRank']= pd.cut(df_oot['proba'], bins=custom_bins)\n",
    "df_stats = pd.DataFrame(df_oot.groupby(by='DecileRank')['ns_flag'].mean())\n",
    "df_stats['volume'] = df_oot.groupby(by='DecileRank')['ns_flag'].count()\n",
    "df_stats['volume%'] = np.round((df_stats['volume'] / df_stats['volume'].sum())*100, 2)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validation(tuned_model, x_train, y_train, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cross_validation(tuned_model, x_test, y_test, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tuned model - increasing class weight to 5 from 7\n",
    "top_features = feature_imp_df[:25]['feature'].to_list()\n",
    "x_tr = x_train[top_features]\n",
    "x_te = x_test[top_features]\n",
    "x_oo = x_oot[top_features]\n",
    "\n",
    "\n",
    "tuned_model6 = XGBClassifier(max_depth=3, colsample_bytree=1 ,subsample=1 ,scale_pos_weight=7 ,learning_rate=0.1 \n",
    "                            ,random_state=3)\n",
    "tuned_model6.fit(x_tr, y_train)\n",
    "\n",
    "print(\"TRAIN RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_tr)\n",
    "predicted_probas = tuned_model6.predict_proba(x_tr)\n",
    "model_metrics(y_pred, y_train, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nTEST RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_te)\n",
    "predicted_probas = tuned_model6.predict_proba(x_te)\n",
    "model_metrics(y_pred, y_test, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nOOT RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_oo)\n",
    "predicted_probas = tuned_model6.predict_proba(x_oo)\n",
    "model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned model - increasing class weight to 5 from 7\n",
    "top_features = feature_imp_df[:25]['feature'].to_list()\n",
    "x_tr = x_train[top_features]\n",
    "x_te = x_test[top_features]\n",
    "x_oo = x_oot[top_features]\n",
    "\n",
    "\n",
    "tuned_model6 = XGBClassifier(max_depth=3, colsample_bytree=1 ,subsample=1 ,scale_pos_weight=6 ,learning_rate=0.1 \n",
    "                            ,random_state=3)\n",
    "tuned_model6.fit(x_tr, y_train)\n",
    "\n",
    "print(\"TRAIN RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_tr)\n",
    "predicted_probas = tuned_model6.predict_proba(x_tr)\n",
    "model_metrics(y_pred, y_train, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nTEST RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_te)\n",
    "predicted_probas = tuned_model6.predict_proba(x_te)\n",
    "model_metrics(y_pred, y_test, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nOOT RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_oo)\n",
    "predicted_probas = tuned_model6.predict_proba(x_oo)\n",
    "model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned model - increasing class weight to 5 from 7\n",
    "top_features = feature_imp_df[:25]['feature'].to_list()\n",
    "x_tr = x_train[top_features]\n",
    "x_te = x_test[top_features]\n",
    "x_oo = x_oot[top_features]\n",
    "\n",
    "\n",
    "tuned_model6 = XGBClassifier(max_depth=3, colsample_bytree=1 ,subsample=1 ,scale_pos_weight=5 ,learning_rate=0.1 \n",
    "                            ,random_state=3)\n",
    "tuned_model6.fit(x_tr, y_train)\n",
    "\n",
    "print(\"TRAIN RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_tr)\n",
    "predicted_probas = tuned_model6.predict_proba(x_tr)\n",
    "model_metrics(y_pred, y_train, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nTEST RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_te)\n",
    "predicted_probas = tuned_model6.predict_proba(x_te)\n",
    "model_metrics(y_pred, y_test, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nOOT RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_oo)\n",
    "predicted_probas = tuned_model6.predict_proba(x_oo)\n",
    "model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned model - increasing class weight to 5 from 7\n",
    "top_features = feature_imp_df[:25]['feature'].to_list()\n",
    "x_tr = x_train[top_features]\n",
    "x_te = x_test[top_features]\n",
    "x_oo = x_oot[top_features]\n",
    "\n",
    "\n",
    "tuned_model6 = XGBClassifier(max_depth=3, colsample_bytree=1 ,subsample=1 ,scale_pos_weight=7 ,learning_rate=0.1 \n",
    "                            ,random_state=3, n_estimators=1000)\n",
    "tuned_model6.fit(x_tr, y_train)\n",
    "\n",
    "print(\"TRAIN RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_tr)\n",
    "predicted_probas = tuned_model6.predict_proba(x_tr)\n",
    "model_metrics(y_pred, y_train, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nTEST RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_te)\n",
    "predicted_probas = tuned_model6.predict_proba(x_te)\n",
    "model_metrics(y_pred, y_test, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nOOT RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_oo)\n",
    "predicted_probas = tuned_model6.predict_proba(x_oo)\n",
    "model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = SQLQuery('snowflake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_v1 = q(\"\"\"\n",
    "                select application_id,\n",
    "                 pred_prob\n",
    "          from \"PROD_DB\".\"DATA\".\"APPLICATIONS\"\n",
    "          where date(application_complete_datetime) between '2022-01-01' and '2023-01-31'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tr = tuned_model6.predict_proba(x_tr)[:,1:].flatten()\n",
    "prob_te = tuned_model6.predict_proba(x_te)[:,1:].flatten()\n",
    "prob_oo = tuned_model6.predict_proba(x_oo)[:,1:].flatten()\n",
    "prob_v2 = np.concatenate([prob_oo,prob_te,prob_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_score, psi_df = novo_ml_metrics.psi_equal_frequency(actual=df_score_v1.pred_prob.values, expected=prob_v2, \n",
    "                                                        plot=True)\n",
    "print(psi_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_score, psi_df = novo_ml_metrics.psi_equal_width(actual=df_score_v1.pred_prob.values, expected=prob_v2, \n",
    "                                                        plot=True)\n",
    "print(psi_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned model\n",
    "tuned_model_final = XGBClassifier(max_depth=3, colsample_bytree=1 ,subsample=1 ,scale_pos_weight=7 ,learning_rate=0.1 \n",
    "                            ,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_features = 398\n",
    "num_features_to_remove = len(independent_features) - max_features\n",
    "\n",
    "for i in range(0, num_features_to_remove):\n",
    "    # train the model\n",
    "    x_train = x_train[independent_features]\n",
    "    tuned_model_final.fit(x_train, y_train)\n",
    "    if (len(independent_features)) == max_features:\n",
    "        break\n",
    "    # get feature importance\n",
    "    imp_df = pd.DataFrame(tuned_model_final.get_booster().get_score(importance_type='total_gain').items(), columns=['feature', 'importance'])\n",
    "    # remove the least important feature\n",
    "    feature_to_remove = imp_df.sort_values(by='importance', ascending=False).iloc[-1]['feature']\n",
    "    print('feature removed:', feature_to_remove)\n",
    "    independent_features.remove(feature_to_remove)\n",
    "print('final features:', independent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN RESULTS:\\n\")\n",
    "y_pred = tuned_model_final.predict(x_train)\n",
    "predicted_probas = tuned_model_final.predict_proba(x_train)\n",
    "model_metrics(y_pred, y_train, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nTEST RESULTS:\\n\")\n",
    "y_pred = tuned_model_final.predict(x_test)\n",
    "predicted_probas = tuned_model_final.predict_proba(x_test)\n",
    "model_metrics(y_pred, y_test, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nOOT RESULTS:\\n\")\n",
    "y_pred = tuned_model_final.predict(x_oot)\n",
    "predicted_probas = tuned_model_final.predict_proba(x_oot)\n",
    "model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape, sum(t), t[84554+36639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_test[y_test==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBClassifier(random_state=3)\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': range (2, 5, 1),\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "#     'subsample': [0.8, 1],\n",
    "#     'colsample_bytree' : [0.8, 1],\n",
    "    'scale_pos_weight' : range(4, 8, 1)\n",
    "}\n",
    "\n",
    "x_train_test = pd.concat([x_train, x_test], axis=0).reset_index(drop=True)\n",
    "y_train_test = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "best_model = tune_hyperparameters(x_train_test, y_train_test, estimator, parameters, metric='roc_auc', cv_folds=3)\n",
    "\n",
    "# best_model.save_model(\"best_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best_model parameters\n",
    "\n",
    "# XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "#               gamma=0, gpu_id=-1, importance_type=None,\n",
    "#               interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
    "#               max_depth=4, min_child_weight=1, missing=nan,\n",
    "#               monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
    "#               num_parallel_tree=1, predictor='auto', random_state=3,\n",
    "#               reg_alpha=0, reg_lambda=1, scale_pos_weight=6, subsample=1,\n",
    "#               tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN RESULTS:\\n\")\n",
    "y_pred = best_model.predict(x_train)\n",
    "predicted_probas = best_model.predict_proba(x_train)\n",
    "model_metrics(y_pred, y_train, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nTEST RESULTS:\\n\")\n",
    "y_pred = best_model.predict(x_test)\n",
    "predicted_probas = best_model.predict_proba(x_test)\n",
    "model_metrics(y_pred, y_test, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nOOT RESULTS:\\n\")\n",
    "y_pred = best_model.predict(x_oot)\n",
    "predicted_probas = best_model.predict_proba(x_oot)\n",
    "model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model2 = XGBClassifier(random_state=3, max_depth=4, scale_pos_weight=6, learning_rate=0.1)\n",
    "best_model2.fit(x_train, y_train)\n",
    "\n",
    "print(\"TRAIN RESULTS:\\n\")\n",
    "y_pred = best_model2.predict(x_train)\n",
    "predicted_probas = best_model2.predict_proba(x_train)\n",
    "model_metrics(y_pred, y_train, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nTEST RESULTS:\\n\")\n",
    "y_pred = best_model2.predict(x_test)\n",
    "predicted_probas = best_model2.predict_proba(x_test)\n",
    "model_metrics(y_pred, y_test, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nOOT RESULTS:\\n\")\n",
    "y_pred = best_model2.predict(x_oot)\n",
    "predicted_probas = best_model2.predict_proba(x_oot)\n",
    "model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_imp_df = pd.DataFrame(zip(x_train.columns.to_list(), best_model2.feature_importances_), columns=['feature','importance'])\n",
    "feature_imp_df = feature_imp_df.sort_values(by=['importance'], ascending=False).reset_index(drop=True)\n",
    "feature_imp_df[feature_imp_df.importance>=0.005]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned model - increasing class weight to 5 from 7\n",
    "top_features = feature_imp_df[:25]['feature'].to_list()\n",
    "x_tr = x_train[top_features]\n",
    "x_te = x_test[top_features]\n",
    "x_oo = x_oot[top_features]\n",
    "\n",
    "\n",
    "tuned_model6 = XGBClassifier(random_state=3, max_depth=4, scale_pos_weight=6, learning_rate=0.1)\n",
    "tuned_model6.fit(x_tr, y_train)\n",
    "\n",
    "print(\"TRAIN RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_tr)\n",
    "predicted_probas = tuned_model6.predict_proba(x_tr)\n",
    "model_metrics(y_pred, y_train, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nTEST RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_te)\n",
    "predicted_probas = tuned_model6.predict_proba(x_te)\n",
    "model_metrics(y_pred, y_test, predicted_probas)\n",
    "\n",
    "print(\"\\n\\nOOT RESULTS:\\n\")\n",
    "y_pred = tuned_model6.predict(x_oo)\n",
    "predicted_probas = tuned_model6.predict_proba(x_oo)\n",
    "model_metrics(y_pred, y_oot, predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Increasing class weight increasing increases recall and decreases precision and otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"nsql_model_v1.pkl\"\n",
    "# path = project_path + 'models/'\n",
    "# pickle.dump(tuned_model, open(path + file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADHOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_excel('../../../../../slack_downloads/Account Closures Jun 1.xlsx')\n",
    "\n",
    "# from sqlalchemy.types import NVARCHAR\n",
    "# # write the new applications to db\n",
    "# test.to_sql(name='testing_fraud_scores',\n",
    "#                  con=q.engine, \n",
    "#                  schema='prod_db.adhoc',\n",
    "#                  if_exists='append', \n",
    "#                  index=False, \n",
    "#                  chunksize=16000, \n",
    "#                  method='multi',\n",
    "#                  dtype={col_name: NVARCHAR for col_name in test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 4)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fa8e7a0e7c7188de72acea4ae1bc222d1770499c4c3d36ce32843ef46b20053"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
