{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path variables\n",
    "import sys\n",
    "project_path = '/Users/naresh/Downloads/DS/growth/nsl_v2/nsl_v2_final/'\n",
    "sys.path.insert(0, project_path+'config')\n",
    "from config import SQLQuery\n",
    "\n",
    "# core libraries\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from model_evaluations import model_metrics, cross_validation\n",
    "from model_building import tune_hyperparameters\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, precision_score, recall_score, roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from stability_monitoring import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# north star customers definition\n",
    "txn_days = 90\n",
    "txn_credit_amount = 15000\n",
    "\n",
    "# from_date = date(2022,10,1)\n",
    "# to_date = date(2023,2,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = SQLQuery('snowflake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query to fetch the required data\n",
    "# df_raw_app = q(\"\"\"with \n",
    "\n",
    "\n",
    "# ALLOY_PERSONS as -- get all the alloy persons data (max of application_versions_id in case of duplicates)\n",
    "# (\n",
    "#   select * from (\n",
    "# select * ,\n",
    "#        row_number() over (partition by APPLICATION_ID order by APPLICATION_VERSION_ID desc) as rank\n",
    "# from \"PROD_DB\".\"DATA\".\"ALLOY_EVALUATIONS_PERSONS\") \n",
    "# where rank=1\n",
    "# ),\n",
    "\n",
    "# APPLICATIONS as -- get all entries from applications table\n",
    "# (\n",
    "#   select *\n",
    "#   from \"PROD_DB\".\"DATA\".\"APPLICATIONS\"\n",
    "#   where application_id is not null\n",
    "#   and date(APPLICATION_COMPLETE_DATETIME) <= '2020-12-31'\n",
    "# )\n",
    "\n",
    "#     select distinct\n",
    "    \n",
    "#     a.application_id,\n",
    "#     a.estimated_monthly_revenue, a.incoming_ach_payments, a.outgoing_ach_and_checks,\n",
    "#     a.check_deposit_amount, a.outgoing_wire_transfers, a.incoming_wire_transfer,\n",
    "#     a.business_type, a.email_domain, a.current_bank, a.industry_category_name,\n",
    "#     b.iovation_device_type, b.iovation_device_timezone, b.carrier, b.socure_sigma, b.socure_phonerisk,\n",
    "#     b.socure_emailrisk, b.socure_reason_code, b.socure_phonerisk_reason_code, b.socure_emailrisk_reason_code\n",
    "    \n",
    "#     from APPLICATIONS a\n",
    "#     left join ALLOY_PERSONS b\n",
    "#     on a.application_id = b.application_id\n",
    "#     \"\"\")\n",
    "\n",
    "# df_raw_app.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop null application ids\n",
    "# df_raw_app = df_raw_app.dropna(subset='application_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop duplicate columns\n",
    "# df_raw_app = df_raw_app.loc[:,~df_raw_app.columns.duplicated()].copy()\n",
    "# df_raw_app.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "file = 'all_apps_till_2020_12_31.pkl'\n",
    "path = project_path + 'data/'\n",
    "# df_raw_app.reset_index(inplace=True)\n",
    "# df_raw_app.to_pickle(path+file)\n",
    "\n",
    "\n",
    "df_raw_app = pd.read_pickle(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_raw_data = q(\n",
    "# \"\"\"\n",
    "# with \n",
    "# APPLICATIONS as -- get all entries from applications table\n",
    "# (\n",
    "#   select *\n",
    "#   from \"PROD_DB\".\"DATA\".\"APPLICATIONS\"\n",
    "#   where application_id is not null\n",
    "#   and date(APPLICATION_COMPLETE_DATETIME) <= '2020-12-31'\n",
    "# )\n",
    "\n",
    "# ,segment_all as (\n",
    "# select b.application_id, a.USER_ID, a.anonymous_id, a.CONTEXT_IP, a.OWNER_ID, context_page_path, screen_width, screen_height, timezone, sent_at, received_at\n",
    "# from APPLICATIONS b\n",
    "# left join SEGMENT_DB.ONBOARDING_PROD.PAGES a\n",
    "# on a.application_id=b.application_id\n",
    "# order by a.application_id, received_at asc\n",
    "# )\n",
    "\n",
    "# -- Pull all the records which crossed the 13th question  \n",
    "# ,segment_till_incoming as (select a.application_id, a.context_page_path, a.received_at from \n",
    "# (select a.application_id, a.context_page_path, a.received_at, rank() over(partition by a.application_id order by a.received_at asc) as rk\n",
    "# from segment_all a\n",
    "# where a.context_page_path='/app/business-questions/incoming'\n",
    "# ) a where rk=1 )\n",
    "\n",
    "# -- Pull customer visited pages till the 13th question\n",
    "# ,final as (select\n",
    "# a.application_id, a.USER_ID, a.anonymous_id, a.CONTEXT_IP, a.OWNER_ID, a.context_page_path, a.screen_width, a.screen_height, a.timezone, a.sent_at, a.received_at \n",
    "# from segment_all a\n",
    "# inner join segment_till_incoming b\n",
    "# on a.application_id=b.application_id and a.received_at <= b.received_at\n",
    "# order by a.application_id, a.received_at asc\n",
    "# )\n",
    "\n",
    "# select * from final order by application_id, received_at asc\n",
    "\n",
    "# \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features_1(df:pd.DataFrame, cols_list:list, app_level_df:pd.DataFrame, training:bool=True):\n",
    "    \"\"\" \n",
    "        df : segment raw data\n",
    "        cols_list: columns list on which the operations are performed\n",
    "        app_level_df: application level data\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    new_cols = []\n",
    "    for col in cols_list:\n",
    "        tmp = df[['application_id',col]].drop_duplicates(subset=['application_id',col], keep='first')\n",
    "        tmp2 = pd.DataFrame(tmp.application_id.value_counts()).reset_index()\n",
    "        col = col+'_count'\n",
    "        tmp2.rename(columns={'application_id':col, 'index':'application_id'}, inplace=True)\n",
    "    \n",
    "        # Merging with app level df\n",
    "        app_level_df = pd.merge(app_level_df, tmp2, on='application_id', how='left')\n",
    "        new_cols.append(col)\n",
    "        \n",
    "    if training:\n",
    "        df_impute_segment = app_level_df[new_cols].median()\n",
    "        df_impute_segment = pd.DataFrame(df_impute_segment, columns=['impute_value']).reset_index().rename(columns={'index':'feature'})\n",
    "        df_impute_segment.to_pickle(project_path+'models/df_impute_segment.pkl') # Save the impute values as df\n",
    "\n",
    "    else:\n",
    "        df_impute_segment = pd.read_pickle(project_path+'models/df_impute_segment.pkl') # Load the impute values as df\n",
    "\n",
    "    # screen height and width features\n",
    "    tmp2 = pd.DataFrame()\n",
    "    tmp2 = pd.concat([tmp2, df[['application_id','screen_height','screen_width']]])\n",
    "    tmp2.drop_duplicates(subset=['application_id','screen_height','screen_width'],keep='first', inplace=True)\n",
    "\n",
    "    new_cols2 = []\n",
    "    list_value_type = ['max','min','mean','median']\n",
    "    for value_type in list_value_type:\n",
    "        colw = 'screen_width'\n",
    "        tmp = pd.DataFrame(tmp2.groupby(['application_id'])[colw].agg([value_type]))\n",
    "        tmp[value_type] = tmp[value_type].astype('float')\n",
    "        tmp.rename(columns={value_type:colw+'_'+value_type}, inplace=True) \n",
    "        app_level_df = pd.merge(app_level_df, tmp, on='application_id', how='left')\n",
    "        \n",
    "        new_cols2.append(colw+'_'+value_type)\n",
    "\n",
    "        colh = 'screen_height'\n",
    "        tmp = pd.DataFrame(tmp2.groupby(['application_id'])[colh].agg([value_type]))\n",
    "        tmp[value_type] = tmp[value_type].astype('float')\n",
    "        tmp.rename(columns={value_type:colh+'_'+value_type}, inplace=True) \n",
    "        app_level_df = pd.merge(app_level_df, tmp, on='application_id', how='left')\n",
    "        \n",
    "        new_cols2.append(colh+'_'+value_type)\n",
    "        \n",
    "    if training:\n",
    "        df_impute_segment2 = app_level_df[new_cols2].median()\n",
    "        df_impute_segment2 = pd.DataFrame(df_impute_segment2, columns=['impute_value']).reset_index().rename(columns={'index':'feature'})\n",
    "        df_impute_segment2.to_pickle(project_path+'models/df_impute_segment2.pkl') # Save the impute values as df\n",
    "\n",
    "    else:\n",
    "        df_impute_segment2 = pd.read_pickle(project_path+'models/df_impute_segment2.pkl') # Load the impute values as df\n",
    "\n",
    "    return app_level_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features_2(df:pd.DataFrame, app_level_df:pd.DataFrame, training:bool=True):\n",
    "    \"\"\" \n",
    "        df : segment raw data\n",
    "        cols_list: columns list on which the operations are performed\n",
    "        app_level_df: application level data\n",
    "        \n",
    "    \"\"\"\n",
    "    # Cleaning the list of owner_ids\n",
    "    def clean_string(x):\n",
    "        if x != None:\n",
    "            x = str(x)\n",
    "            x = [val.strip(\"\"\"'|[| |\"|,|]\"\"\") for val in x.split('/') if not val.strip(\"\"\"'|[| |\"|,|]\"\"\") in ['',None] and len(val.strip(\"\"\"'|[| |\"|,|]\"\"\"))<36]\n",
    "        return '/'.join(x)\n",
    "\n",
    "    df['context_page_path_clean'] = df['context_page_path'].apply(clean_string)\n",
    "    # One-hot encoding the unique page paths\n",
    "    col = 'context_page_path_clean'\n",
    "    tmp = df[col].str.get_dummies()\n",
    "    tmp['application_id'] = df.application_id\n",
    "    # Common pages that every applicant must visit\n",
    "    common_pages = ['verify-email-otp','welcome','app/applicant/personal-info','app/applicant/phone'\n",
    "    ,'app/applicant/otp-verify','app/applicant/address','app/applicant/dob-ssn','app/business/business-type'\n",
    "    ,'app/business/address','app/business/other-info','app/business-questions/about-business'\n",
    "    ,'app/business-questions/incoming']\n",
    "\n",
    "    # no.of visits per page\n",
    "    tmp2 = tmp[common_pages+['application_id']].groupby(['application_id']).sum()\n",
    "    tmp2.reset_index(drop=False, inplace=True)\n",
    "    tmp2 = pd.merge(app_level_df, tmp2, on='application_id', how='left')\n",
    "    \n",
    "    # no.of unique pages per user\n",
    "    tmp3 = tmp.groupby(['application_id']).sum()\n",
    "    tmp3.reset_index(drop=False, inplace=True)\n",
    "    tmp3 = pd.merge(tmp3, app_level_df[['application_id']], on='application_id', how='left')\n",
    "    drop_cols = ['404','app/applicant','app/business','application-denied','forgot-password','signup','status','undefined']\n",
    "    drop_cols = [col for col in tmp3.columns.to_list() if col in drop_cols]\n",
    "    tmp3 = tmp3.drop(columns=drop_cols)\n",
    "    tmp3 = tmp3.dropna(subset=['application_id'])\n",
    "    \n",
    "    tmp4 = pd.DataFrame()\n",
    "    tmp4['application_id'] = tmp3.application_id\n",
    "    cols = tmp3.drop(columns=['application_id']).columns.to_list()\n",
    "    tmp4[cols] = pd.DataFrame(np.where(tmp3[cols]>=1, 1, 0))\n",
    "    tmp4 = tmp4.set_index('application_id')\n",
    "    tmp4 = pd.DataFrame(tmp4.sum(axis=1), columns=['page_count'])\n",
    "    tmp2 = pd.merge(tmp2,tmp4,on='application_id',how='left')\n",
    "\n",
    "    if training:\n",
    "        df_impute_segment3 = tmp2[common_pages+['page_count']].median()\n",
    "        df_impute_segment3 = pd.DataFrame(df_impute_segment3, columns=['impute_value']).reset_index().rename(columns={'index':'feature'})\n",
    "        df_impute_segment3.to_pickle(project_path+'models/df_impute_segment3.pkl') # Save the impute values as df\n",
    "\n",
    "    else:\n",
    "        df_impute_segment3 = pd.read_pickle(project_path+'models/df_impute_segment3.pkl') # Load the impute values as df\n",
    "\n",
    "    return tmp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features_3(app_level_df:pd.DataFrame):\n",
    "    \"\"\" \n",
    "        df : segment raw data\n",
    "        cols_list: columns list on which the operations are performed\n",
    "        app_level_df: application level data\n",
    "        \n",
    "    \"\"\"\n",
    "    impute1 = pd.read_pickle(project_path+'models/df_impute_segment.pkl')\n",
    "    impute2 = pd.read_pickle(project_path+'models/df_impute_segment2.pkl')\n",
    "    impute3 = pd.read_pickle(project_path+'models/df_impute_segment3.pkl')\n",
    "    \n",
    "    ############# PART-1 #############\n",
    "    # Function to fill nulls with median values\n",
    "    def fill_null_values(df_impute:pd.DataFrame, data_df:pd.DataFrame,):\n",
    "        df_dict = dict(df_impute.values)\n",
    "        impute_cols = df_impute['feature'].to_list()\n",
    "        for col in data_df.columns.to_list():\n",
    "            if col in impute_cols:\n",
    "                data_df[col] = data_df[col].fillna(df_dict[col])\n",
    "                impute_cols.remove(col)\n",
    "        return data_df\n",
    "\n",
    "    # Filling nulls with median\n",
    "    app_level_df = fill_null_values(impute1, app_level_df)\n",
    "    app_level_df['sh_sw_ratio_count'] = app_level_df['screen_height_count']/app_level_df['screen_width_count']\n",
    "    app_level_df['sh_sw_ratio_count'] = app_level_df['sh_sw_ratio_count'].astype('float')\n",
    "    \n",
    "    cols_list_1 = impute1.feature.to_list()\n",
    "    # Feature Engg\n",
    "    for col in cols_list_1:\n",
    "        app_level_df[col] = np.where(app_level_df[col]==1,1,0)\n",
    "        \n",
    "\n",
    "    ############# PART-2 #############\n",
    "    # Filling nulls with median\n",
    "    app_level_df = fill_null_values(impute2, app_level_df)\n",
    "\n",
    "    # Function to fill zero screen width and height with median values\n",
    "    def fill_zero_values(df_impute:pd.DataFrame, data_df:pd.DataFrame,):\n",
    "        df_dict = dict(df_impute.values)\n",
    "        impute_cols = df_impute['feature'].to_list()\n",
    "        for col in data_df.columns.to_list():\n",
    "            if col in impute_cols:\n",
    "                data_df[col] = np.where(data_df[col]==0, df_dict[col], data_df[col])\n",
    "                impute_cols.remove(col)\n",
    "        return data_df\n",
    "    \n",
    "    # Filling zeros with median\n",
    "    app_level_df = fill_zero_values(impute2, app_level_df)\n",
    "    # Creating Ratios\n",
    "    list_value_type = ['max','min','mean','median']\n",
    "    for value_type in list_value_type:\n",
    "        colh = 'screen_height'\n",
    "        colw = 'screen_width'\n",
    "        app_level_df['sh_sw_ratio_'+value_type] = app_level_df[colh+'_'+value_type]/app_level_df[colw+'_'+value_type]\n",
    "\n",
    "    \n",
    "    ############# PART-3 #############\n",
    "    cols_list_3 = impute3.feature.to_list()\n",
    "    app_level_df = fill_null_values(impute3, app_level_df)\n",
    "    \n",
    "    return app_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# segment_pages_oot = segment_raw_data.copy()\n",
    "# # Filter only the existing applications in segment data\n",
    "# segment_pages_oot = segment_pages_oot[~segment_pages_oot.received_at.isnull()]\n",
    "# segment_pages_oot.reset_index(drop=True, inplace=True)\n",
    "# segment_pages_oot = segment_pages_oot.sort_values(by=['application_id','received_at'])\n",
    "\n",
    "# # Change data type\n",
    "# cols = ['application_id','user_id','owner_id','anonymous_id','context_page_path','timezone']\n",
    "# segment_pages_oot[cols] = segment_pages_oot[cols].astype('string')\n",
    "# segment_pages_oot[['screen_width','screen_height']] = segment_pages_oot[['screen_width','screen_height']].astype('int')\n",
    "\n",
    "# for col in segment_pages_oot.columns:\n",
    "#     if col != 'user_id':\n",
    "#         idx = segment_pages_oot.index[segment_pages_oot[col].isnull()].tolist()\n",
    "#         idx.extend(segment_pages_oot.index[segment_pages_oot[col].isna()].tolist())\n",
    "#         idx.extend(segment_pages_oot.index[segment_pages_oot[col] == ''].tolist())\n",
    "#         idx.extend(segment_pages_oot.index[segment_pages_oot[col] == '[]'].tolist())\n",
    "#         idx = list(set(idx))\n",
    "#         segment_pages_oot.loc[idx, col] = None    \n",
    "\n",
    "\n",
    "# df_oot = df_raw_app.copy()\n",
    "\n",
    "# df_oot_tmp = pd.merge(segment_pages_oot, df_oot[['application_id']], on='application_id', how='inner')\n",
    "# x_oot = df_oot_tmp.reset_index(drop=True)\n",
    "\n",
    "# cols_list = ['timezone','user_id','owner_id','anonymous_id','context_ip','screen_width','screen_height']\n",
    "# # Creating df with all apps\n",
    "# app_level_data = pd.DataFrame()\n",
    "# app_level_data['application_id'] = x_oot.application_id.unique()\n",
    "\n",
    "# app_level_data = segment_features_1(df=x_oot, cols_list=cols_list, app_level_df=app_level_data, training=False)\n",
    "# app_level_data = segment_features_2(df=x_oot, app_level_df=app_level_data, training=False)\n",
    "\n",
    "# df_oot = pd.merge(df_oot[['application_id']], app_level_data, on='application_id', how='left')\n",
    "# x_oot = df_oot.reset_index(drop=True)\n",
    "# app_level_data_oot = segment_features_3(app_level_df=x_oot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_level_data.shape[0], segment_raw_data.application_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the oot dataset\n",
    "file = 'all_apps_segment_till_2020_12_31.pkl'\n",
    "path = project_path + 'data/'\n",
    "# app_level_data_oot.reset_index(inplace=True)\n",
    "# app_level_data_oot.to_pickle(path+file)\n",
    "\n",
    "app_level_data_oot = pd.read_pickle(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nulls_to_one_format(df:pd.DataFrame):\n",
    "    for col in df.columns:\n",
    "        idx = df.index[df[col].isnull()].tolist()\n",
    "        idx.extend(df.index[df[col].isna()].tolist())\n",
    "        idx.extend(df.index[df[col] == ''].tolist())\n",
    "        idx.extend(df.index[df[col] == '[]'].tolist())\n",
    "        idx = list(set(idx))\n",
    "        df.loc[idx, col] = None\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_null_values(df_impute:pd.DataFrame, data_df:pd.DataFrame,):\n",
    "    df_dict = dict(df_impute.values)\n",
    "    impute_cols = df_impute['feature'].to_list()\n",
    "    for col in data_df.columns.to_list():\n",
    "        if col in impute_cols:\n",
    "            data_df[col] = data_df[col].fillna(df_dict[col])\n",
    "            impute_cols.remove(col)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_alloy(df:pd.DataFrame, training:bool=True):\n",
    "    \n",
    "    # feature 1\n",
    "    col = 'iovation_device_type'\n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = np.where(df[col].isin(['windows','iphone','mac','android']),df[col],'other')\n",
    "    \n",
    "    # feature 3\n",
    "    df['iovation_device_timezone'] = np.where(df['iovation_device_timezone'].isin(['300','360','480']),\n",
    "                                                   df['iovation_device_timezone'], 'other')\n",
    "    # feature 6 \n",
    "    col = 'carrier'\n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = np.where(df[col].str.contains('verizon'),'verizon',df[col])\n",
    "    df[col] = np.where(df[col].str.contains('at&t'),'att',df[col])\n",
    "    df[col] = np.where(df[col].str.match('att'),'att',df[col])\n",
    "    df[col] = np.where(df[col].str.contains('t-mobile*'),'tmobile',df[col])\n",
    "    df[col] = np.where(df[col].isin(['att','tmobile','verizon']),df[col],'other')\n",
    "    \n",
    "    # Socure reason codes\n",
    "    # Cleaning the list of reason codes\n",
    "    def clean_string(x):\n",
    "        if x != None:\n",
    "            x = [val.strip(\"\"\"'|[| |\"|,|]\"\"\") for val in x.split('\\n') if not val.strip(\"\"\"'|[| |\"|,|]\"\"\") in ['',None]]\n",
    "        return x\n",
    "\n",
    "    socure_cols = ['socure_phonerisk_reason_code','socure_emailrisk_reason_code','socure_reason_code']\n",
    "    df_socure = df[socure_cols]\n",
    "\n",
    "    # Collecting the cleaned reason codes\n",
    "    for col in socure_cols:\n",
    "        df_socure[col] = df_socure[col].astype('str')\n",
    "        df_socure[col] = df_socure[col].str.lower()\n",
    "        df_socure[col] = df_socure[col].apply(clean_string)\n",
    "        \n",
    "    dict_cols = {}\n",
    "    for col in socure_cols:\n",
    "        df_socure = df_socure.drop(col, 1).join(df_socure[col].str.join('|').str.get_dummies())\n",
    "        socure_cols = df_socure.columns[df_socure.columns.str.startswith('socure')].to_list()\n",
    "        true_cols = list(set(df_socure.columns.to_list()) - set(socure_cols))\n",
    "        new_cols = []\n",
    "        for col2 in true_cols:\n",
    "            new_cols.append(col+'_'+col2)\n",
    "        dict_cols = dict(zip(true_cols, new_cols)) | dict_cols\n",
    "        df_socure.rename(columns=dict_cols, inplace=True)\n",
    "\n",
    "    df_socure = df_socure.T\n",
    "    df_socure = df_socure[~df_socure.index.duplicated(keep='first')].T\n",
    "    df_socure = df_socure.astype('int')\n",
    "        \n",
    "    socure_reason_codes_columns = pd.read_pickle(project_path+'models/socure_reason_codes_columns.pkl')\n",
    "    df_tmp = pd.DataFrame(index=range(df.shape[0]),columns=socure_reason_codes_columns['feature'].to_list())\n",
    "    df_tmp = df_tmp.fillna(0)\n",
    "    \n",
    "    df_tmp.update(df_socure)\n",
    "    df_tmp = df_tmp.astype('int')\n",
    "    df = pd.concat([df,df_tmp], axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_app(df:pd.DataFrame):\n",
    "    # business type\n",
    "    df['business_group'] = np.where(df['business_type'] == 'sole_proprietorship', 0, 1)\n",
    "\n",
    "    # email domain\n",
    "    email_domain_group = pd.read_pickle(project_path+'models/email_domain_group.pkl')['email_domain'].to_list()\n",
    "    df['email_domain_bucket'] = np.where(df['email_domain'].isin(email_domain_group), 0, 1)\n",
    "\n",
    "    # estimated business numbers\n",
    "    estimated_cols = ['estimated_monthly_revenue',\n",
    "                      'incoming_ach_payments',\n",
    "                      'check_deposit_amount',\n",
    "                      'incoming_wire_transfer',\n",
    "                      'outgoing_ach_and_checks',\n",
    "                      'outgoing_wire_transfers']\n",
    "\n",
    "    # grouping all responses into 5K+ and 5K-\n",
    "    for col in estimated_cols:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = np.where(df[col].isin(['$5k +', '$50k +']), 1, 0)\n",
    "\n",
    "    # current bank\n",
    "    hdb_group = ['bluevine', 'other-national-bank', 'td-ank', 'chase', 'usaa']\n",
    "    df['current_bank_group'] = np.where(df['current_bank'].isin(hdb_group), 1, 0)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_features_app = [\n",
    " 'email_domain',\n",
    " 'estimated_monthly_revenue',\n",
    " 'incoming_ach_payments',\n",
    " 'check_deposit_amount',\n",
    " 'incoming_wire_transfer',\n",
    " 'outgoing_ach_and_checks',\n",
    " 'outgoing_wire_transfers',\n",
    " 'current_bank',\n",
    " 'industry_category_name',\n",
    " 'business_type'\n",
    "]\n",
    "\n",
    "# Alloy Features\n",
    "# Numerical\n",
    "num_cols = ['socure_emailrisk', 'socure_phonerisk','socure_sigma']\n",
    "# Categorical\n",
    "cat_columns = ['iovation_device_type','iovation_device_timezone','carrier']\n",
    "# Reason codes - Socure\n",
    "socure_reason_cols = ['socure_reason_code','socure_emailrisk_reason_code','socure_phonerisk_reason_code']\n",
    "raw_features_segment = ['screen_width_mean','sh_sw_ratio_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_features_alloy = num_cols + cat_columns + socure_reason_cols\n",
    "raw_features = raw_features_app + raw_features_alloy + raw_features_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_features_app = ['estimated_monthly_revenue', \n",
    "                            'incoming_ach_payments', \n",
    "                            'check_deposit_amount', \n",
    "                            'incoming_wire_transfer',\n",
    "                            'outgoing_ach_and_checks', \n",
    "                            'outgoing_wire_transfers',\n",
    "                            'business_group', \n",
    "                            'email_domain_bucket', \n",
    "                            'industry_category_name',\n",
    "                            'current_bank_group'\n",
    "                           ]\n",
    "\n",
    "# Alloy Features\n",
    "reason_codes_cols = pd.read_pickle(project_path+'models/socure_reason_codes_columns.pkl'\n",
    "                                            )['feature'].to_list()\n",
    "# Final alloy columns\n",
    "independent_features_alloy_tmp = num_cols + cat_columns + reason_codes_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oot_df = df_raw_app.copy()\n",
    "\n",
    "# Adding segment test data\n",
    "segment_oot = app_level_data_oot.copy()\n",
    "oot_df = pd.merge(oot_df, segment_oot, on='application_id', how='inner')\n",
    "x_oot = oot_df[raw_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert all string features to lowercase\n",
    "string_features = ['email_domain',\n",
    " 'current_bank',\n",
    " 'industry_category_name',\n",
    " 'business_type']+cat_columns+socure_reason_cols\n",
    "\n",
    "for col in string_features:\n",
    "    x_oot[col] = x_oot[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72026, 21)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_oot = convert_nulls_to_one_format(df=x_oot)\n",
    "df_impute = pd.read_pickle(project_path+'models/df_impute.pkl')\n",
    "x_oot = fill_null_values(df_impute, x_oot)\n",
    "\n",
    "x_oot = feature_engineering_app(df=x_oot)\n",
    "x_oot = feature_engineering_alloy(df=x_oot, training=False)\n",
    "\n",
    "# Removing reason code features as these are already encoded\n",
    "x_oot.drop(columns=socure_reason_cols, inplace=True)\n",
    "\n",
    "# Encoding the categories\n",
    "x_object_cols = x_oot[independent_features_alloy_tmp].select_dtypes(include=['object']).columns.to_list()\n",
    "x_object_cols = x_object_cols+['industry_category_name']\n",
    "\n",
    "x_object_onehot = pd.get_dummies(x_oot[x_object_cols]) # create dummies\n",
    "x_object_onehot = x_object_onehot.astype('int')\n",
    "x_oot = pd.concat([x_oot.drop(columns=x_object_cols), x_object_onehot], axis=1)\n",
    "x_oot.columns= x_oot.columns.str.lower() # convert column names to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72026, 30)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the final independent features\n",
    "# independent_features_alloy = num_cols + x_object_onehot.columns.to_list() + reason_codes_cols\n",
    "# independent_features = independent_features_app + independent_features_alloy + raw_features_segment\n",
    "\n",
    "# x_oot = x_oot[independent_features]\n",
    "\n",
    "# train_cols = pd.read_pickle(project_path+'models/train_data_columns.pkl')['feature'].to_list()\n",
    "train_cols = [\n",
    " 'estimated_monthly_revenue',\n",
    " 'incoming_ach_payments',\n",
    " 'sh_sw_ratio_mean',\n",
    " 'screen_width_mean',\n",
    " 'industry_category_name_professional, scientific, and technical services',\n",
    " 'business_group',\n",
    " 'outgoing_ach_and_checks',\n",
    " 'socure_sigma',\n",
    " 'iovation_device_type_mac',\n",
    " 'industry_category_name_real estate rental and leasing',\n",
    " 'socure_emailrisk',\n",
    " 'socure_emailrisk_reason_code_i566',\n",
    " 'socure_phonerisk',\n",
    " 'industry_category_name_retail trade',\n",
    " 'socure_emailrisk_reason_code_i553',\n",
    " 'iovation_device_type_android',\n",
    " 'outgoing_wire_transfers',\n",
    " 'socure_emailrisk_reason_code_r561',\n",
    " 'check_deposit_amount',\n",
    " 'socure_phonerisk_reason_code_i630',\n",
    " 'socure_reason_code_r207',\n",
    " 'socure_phonerisk_reason_code_i614',\n",
    " 'iovation_device_timezone_480',\n",
    " 'industry_category_name_administrative and support and waste management and remediation services',\n",
    " 'socure_phonerisk_reason_code_r616',\n",
    " 'email_domain_bucket',\n",
    " 'incoming_wire_transfer',\n",
    " 'industry_category_name_health care and social assistance',\n",
    " 'socure_phonerisk_reason_code_r639',\n",
    " 'carrier_tmobile'\n",
    "]\n",
    "\n",
    "df_tmp = pd.DataFrame(index=range(x_oot.shape[0]),columns=train_cols)\n",
    "df_tmp = df_tmp.fillna(0)\n",
    "df_tmp.update(x_oot)\n",
    "x_oot = df_tmp.copy()\n",
    "x_oot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################\n",
    "# depth = 4\n",
    "# weight = 6\n",
    "# file_name = 'nsql_model_depth_'+str(depth)+'_weight_'+str(weight)+'.pkl'\n",
    "# path = project_path + 'models/'\n",
    "# xgb_model = pickle.load(open(project_path+file_name, \"rb\"))\n",
    "\n",
    "xgb_model = pickle.load(open('../../nsl_v2_model_deployment/models/nsql_model_v2.pkl', \"rb\"))\n",
    "\n",
    "top_features = [\n",
    " 'estimated_monthly_revenue',\n",
    " 'incoming_ach_payments',\n",
    " 'sh_sw_ratio_mean',\n",
    " 'screen_width_mean',\n",
    " 'industry_category_name_professional, scientific, and technical services',\n",
    " 'business_group',\n",
    " 'outgoing_ach_and_checks',\n",
    " 'socure_sigma',\n",
    " 'iovation_device_type_mac',\n",
    " 'industry_category_name_real estate rental and leasing',\n",
    " 'socure_emailrisk',\n",
    " 'socure_emailrisk_reason_code_i566',\n",
    " 'socure_phonerisk',\n",
    " 'industry_category_name_retail trade',\n",
    " 'socure_emailrisk_reason_code_i553',\n",
    " 'iovation_device_type_android',\n",
    " 'outgoing_wire_transfers',\n",
    " 'socure_emailrisk_reason_code_r561',\n",
    " 'check_deposit_amount',\n",
    " 'socure_phonerisk_reason_code_i630',\n",
    " 'socure_reason_code_r207',\n",
    " 'socure_phonerisk_reason_code_i614',\n",
    " 'iovation_device_timezone_480',\n",
    " 'industry_category_name_administrative and support and waste management and remediation services',\n",
    " 'socure_phonerisk_reason_code_r616',\n",
    " 'email_domain_bucket',\n",
    " 'incoming_wire_transfer',\n",
    " 'industry_category_name_health care and social assistance',\n",
    " 'socure_phonerisk_reason_code_r639',\n",
    " 'carrier_tmobile'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(x_oot[top_features])\n",
    "predicted_probas = xgb_model.predict_proba(x_oot[top_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oot['nsl_v2_prob'] = predicted_probas[:,1]\n",
    "x_oot['nsl_v2_flag'] = np.where((x_oot['nsl_v2_prob']>0.3) & (x_oot.business_group==1), 1, \n",
    "                               np.where((x_oot['nsl_v2_prob']>0.5) & (x_oot.business_group==0), 1, 0)\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72026, 32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw_app['nsl_v2_prob'] = x_oot['nsl_v2_prob']\n",
    "df_raw_app['nsl_v2_flag'] = x_oot['nsl_v2_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_write = df_raw_app[['application_id','nsl_v2_prob','nsl_v2_flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_write.to_pickle(project_path+'results/part1_2020_12_31.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy.types import NVARCHAR\n",
    "# from conf.config import SQLQuery\n",
    "# q = SQLQuery('snowflake')\n",
    "\n",
    "# df_write.to_sql(name='nsl_v2_scores_part1',\n",
    "#                  con=q.engine, \n",
    "#                  schema='prod_db.adhoc',\n",
    "#                  if_exists='append', \n",
    "#                  index=False, \n",
    "#                  chunksize=16000, \n",
    "#                  method='multi',\n",
    "#                  dtype={col_name: NVARCHAR for col_name in df_write})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fa8e7a0e7c7188de72acea4ae1bc222d1770499c4c3d36ce32843ef46b20053"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
